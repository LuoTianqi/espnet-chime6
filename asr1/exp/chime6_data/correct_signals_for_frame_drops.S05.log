# Running on compute0165
# Started at Tue Apr 14 16:59:28 EDT 2020
# SLURMD_NODENAME=compute0165
# SLURM_CHECKPOINT_IMAGE_DIR=/var/slurm/checkpoint
# SLURM_CLUSTER_NAME=marcc
# SLURM_CPUS_ON_NODE=1
# SLURM_EXPORT_ENV=PATH
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=43188432
# SLURM_JOB_ACCOUNT=swatana4
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=1370
# SLURM_JOB_ID=43188432
# SLURM_JOB_NAME=correct_signals_for_frame_drops.S05.sh
# SLURM_JOB_NODELIST=compute0165
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=shared
# SLURM_JOB_QOS=normal
# SLURM_JOB_UID=4591
# SLURM_JOB_USER=tluo9@jhu.edu
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=4916
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=compute0165
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=24
# SLURM_NTASKS=24
# SLURM_NTASKS_PER_NODE=1
# SLURM_OPEN_MODE=a
# SLURM_PROCID=0
# SLURM_SUBMIT_DIR=/scratch/groups/swatana4/tluo/espnet-4-13/egs/chime6/asr1/chime6-synchronisation
# SLURM_SUBMIT_HOST=gpu049
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=18978
# SLURM_TOPOLOGY_ADDR=compute0165
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=marcc:bc-mgmt:6817:8192
# /home-4/tluo9@jhu.edu/miniconda3/bin/python correct_signals_for_frame_drops.py --session=S05 chime6_audio_edits.json /data/swatana4/CHiME5/audio /data/swatana4/CHiME6/audio_tmp 
WARNING: device U04 missing for session S05
# Accounting: begin_time=1586897968
# Accounting: end_time=1586898196
# Accounting: time=228 threads=1
# Finished at Tue Apr 14 17:03:16 EDT 2020 with status 0
